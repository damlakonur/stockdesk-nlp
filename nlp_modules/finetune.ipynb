{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from torch import cuda\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('damla/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>rt</th>\n",
       "      <th>fav</th>\n",
       "      <th>followers</th>\n",
       "      <th>verified</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221222</td>\n",
       "      <td>Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...</td>\n",
       "      <td>2022-07-10 10:16:06+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>20</td>\n",
       "      <td>343</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>Ger√ßek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221223</td>\n",
       "      <td>en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&amp;gt...</td>\n",
       "      <td>2022-07-09 20:53:12+00:00</td>\n",
       "      <td>ASIM_YALCINKAYA</td>\n",
       "      <td>107</td>\n",
       "      <td>934</td>\n",
       "      <td>138759</td>\n",
       "      <td>False</td>\n",
       "      <td>Ger√ßek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221225</td>\n",
       "      <td>üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...</td>\n",
       "      <td>2022-07-09 07:01:24+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>23</td>\n",
       "      <td>379</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>Ger√ßek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221226</td>\n",
       "      <td>üìçFortune 500 listesinde 2020 ve 2021‚Äôde yer al...</td>\n",
       "      <td>2022-07-08 13:02:50+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>62</td>\n",
       "      <td>665</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>Ger√ßek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221227</td>\n",
       "      <td>Bi tatile gidip geldik 4-5 g√ºn neler olmu≈ü √∂yl...</td>\n",
       "      <td>2022-07-08 07:14:06+00:00</td>\n",
       "      <td>____PASA____</td>\n",
       "      <td>79</td>\n",
       "      <td>1647</td>\n",
       "      <td>111568</td>\n",
       "      <td>False</td>\n",
       "      <td>Ger√ßek</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  \\\n",
       "0  221222  Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...   \n",
       "1  221223  en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&gt...   \n",
       "2  221225  üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...   \n",
       "3  221226  üìçFortune 500 listesinde 2020 ve 2021‚Äôde yer al...   \n",
       "4  221227  Bi tatile gidip geldik 4-5 g√ºn neler olmu≈ü √∂yl...   \n",
       "\n",
       "                        date             user   rt   fav  followers  verified  \\\n",
       "0  2022-07-10 10:16:06+00:00  ibrahim___ethem   20   343     136873     False   \n",
       "1  2022-07-09 20:53:12+00:00  ASIM_YALCINKAYA  107   934     138759     False   \n",
       "2  2022-07-09 07:01:24+00:00  ibrahim___ethem   23   379     136873     False   \n",
       "3  2022-07-08 13:02:50+00:00  ibrahim___ethem   62   665     136873     False   \n",
       "4  2022-07-08 07:14:06+00:00     ____PASA____   79  1647     111568     False   \n",
       "\n",
       "    label  \n",
       "0  Ger√ßek  \n",
       "1  Ger√ßek  \n",
       "2  Ger√ßek  \n",
       "3  Ger√ßek  \n",
       "4  Ger√ßek  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1222, 1136)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.label == 'Ger√ßek']), len(df[df.label == 'Yargƒ±'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df.drop(['label'], axis=1), df['label'].str.get_dummies()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>rt</th>\n",
       "      <th>fav</th>\n",
       "      <th>followers</th>\n",
       "      <th>verified</th>\n",
       "      <th>Ger√ßek</th>\n",
       "      <th>Yargƒ±</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221222</td>\n",
       "      <td>Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...</td>\n",
       "      <td>2022-07-10 10:16:06+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>20</td>\n",
       "      <td>343</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221223</td>\n",
       "      <td>en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&amp;gt...</td>\n",
       "      <td>2022-07-09 20:53:12+00:00</td>\n",
       "      <td>ASIM_YALCINKAYA</td>\n",
       "      <td>107</td>\n",
       "      <td>934</td>\n",
       "      <td>138759</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221225</td>\n",
       "      <td>üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...</td>\n",
       "      <td>2022-07-09 07:01:24+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>23</td>\n",
       "      <td>379</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221226</td>\n",
       "      <td>üìçFortune 500 listesinde 2020 ve 2021‚Äôde yer al...</td>\n",
       "      <td>2022-07-08 13:02:50+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>62</td>\n",
       "      <td>665</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221227</td>\n",
       "      <td>Bi tatile gidip geldik 4-5 g√ºn neler olmu≈ü √∂yl...</td>\n",
       "      <td>2022-07-08 07:14:06+00:00</td>\n",
       "      <td>____PASA____</td>\n",
       "      <td>79</td>\n",
       "      <td>1647</td>\n",
       "      <td>111568</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  \\\n",
       "0  221222  Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...   \n",
       "1  221223  en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&gt...   \n",
       "2  221225  üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...   \n",
       "3  221226  üìçFortune 500 listesinde 2020 ve 2021‚Äôde yer al...   \n",
       "4  221227  Bi tatile gidip geldik 4-5 g√ºn neler olmu≈ü √∂yl...   \n",
       "\n",
       "                        date             user   rt   fav  followers  verified  \\\n",
       "0  2022-07-10 10:16:06+00:00  ibrahim___ethem   20   343     136873     False   \n",
       "1  2022-07-09 20:53:12+00:00  ASIM_YALCINKAYA  107   934     138759     False   \n",
       "2  2022-07-09 07:01:24+00:00  ibrahim___ethem   23   379     136873     False   \n",
       "3  2022-07-08 13:02:50+00:00  ibrahim___ethem   62   665     136873     False   \n",
       "4  2022-07-08 07:14:06+00:00     ____PASA____   79  1647     111568     False   \n",
       "\n",
       "   Ger√ßek  Yargƒ±  \n",
       "0       1      0  \n",
       "1       1      0  \n",
       "2       1      0  \n",
       "3       1      0  \n",
       "4       1      0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>rt</th>\n",
       "      <th>fav</th>\n",
       "      <th>followers</th>\n",
       "      <th>verified</th>\n",
       "      <th>Ger√ßek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221222</td>\n",
       "      <td>Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...</td>\n",
       "      <td>2022-07-10 10:16:06+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>20</td>\n",
       "      <td>343</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221223</td>\n",
       "      <td>en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&amp;gt...</td>\n",
       "      <td>2022-07-09 20:53:12+00:00</td>\n",
       "      <td>ASIM_YALCINKAYA</td>\n",
       "      <td>107</td>\n",
       "      <td>934</td>\n",
       "      <td>138759</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221225</td>\n",
       "      <td>üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...</td>\n",
       "      <td>2022-07-09 07:01:24+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>23</td>\n",
       "      <td>379</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221226</td>\n",
       "      <td>üìçFortune 500 listesinde 2020 ve 2021‚Äôde yer al...</td>\n",
       "      <td>2022-07-08 13:02:50+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>62</td>\n",
       "      <td>665</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221227</td>\n",
       "      <td>Bi tatile gidip geldik 4-5 g√ºn neler olmu≈ü √∂yl...</td>\n",
       "      <td>2022-07-08 07:14:06+00:00</td>\n",
       "      <td>____PASA____</td>\n",
       "      <td>79</td>\n",
       "      <td>1647</td>\n",
       "      <td>111568</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  \\\n",
       "0  221222  Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...   \n",
       "1  221223  en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&gt...   \n",
       "2  221225  üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...   \n",
       "3  221226  üìçFortune 500 listesinde 2020 ve 2021‚Äôde yer al...   \n",
       "4  221227  Bi tatile gidip geldik 4-5 g√ºn neler olmu≈ü √∂yl...   \n",
       "\n",
       "                        date             user   rt   fav  followers  verified  \\\n",
       "0  2022-07-10 10:16:06+00:00  ibrahim___ethem   20   343     136873     False   \n",
       "1  2022-07-09 20:53:12+00:00  ASIM_YALCINKAYA  107   934     138759     False   \n",
       "2  2022-07-09 07:01:24+00:00  ibrahim___ethem   23   379     136873     False   \n",
       "3  2022-07-08 13:02:50+00:00  ibrahim___ethem   62   665     136873     False   \n",
       "4  2022-07-08 07:14:06+00:00     ____PASA____   79  1647     111568     False   \n",
       "\n",
       "   Ger√ßek  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop(['Yargƒ±'], axis=1, inplace=True)\n",
    "# 1 for Ger√ßek 0 for Yargƒ±\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>rt</th>\n",
       "      <th>fav</th>\n",
       "      <th>followers</th>\n",
       "      <th>verified</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221222</td>\n",
       "      <td>Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...</td>\n",
       "      <td>2022-07-10 10:16:06+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>20</td>\n",
       "      <td>343</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221223</td>\n",
       "      <td>en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&amp;gt...</td>\n",
       "      <td>2022-07-09 20:53:12+00:00</td>\n",
       "      <td>ASIM_YALCINKAYA</td>\n",
       "      <td>107</td>\n",
       "      <td>934</td>\n",
       "      <td>138759</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221225</td>\n",
       "      <td>üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...</td>\n",
       "      <td>2022-07-09 07:01:24+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>23</td>\n",
       "      <td>379</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221226</td>\n",
       "      <td>üìçFortune 500 listesinde 2020 ve 2021‚Äôde yer al...</td>\n",
       "      <td>2022-07-08 13:02:50+00:00</td>\n",
       "      <td>ibrahim___ethem</td>\n",
       "      <td>62</td>\n",
       "      <td>665</td>\n",
       "      <td>136873</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221227</td>\n",
       "      <td>Bi tatile gidip geldik 4-5 g√ºn neler olmu≈ü √∂yl...</td>\n",
       "      <td>2022-07-08 07:14:06+00:00</td>\n",
       "      <td>____PASA____</td>\n",
       "      <td>79</td>\n",
       "      <td>1647</td>\n",
       "      <td>111568</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  \\\n",
       "0  221222  Bloomberg Businessweek‚Äôin 3 Temmuz tarihli √∂ze...   \n",
       "1  221223  en UCUZ #BIST 100 #hisse'leri\\n#SAHOL 18,68&gt...   \n",
       "2  221225  üìçƒ∞stanbul Havalimanƒ± d√ºn tarihinin en y√ºksek u...   \n",
       "3  221226  üìçFortune 500 listesinde 2020 ve 2021‚Äôde yer al...   \n",
       "4  221227  Bi tatile gidip geldik 4-5 g√ºn neler olmu≈ü √∂yl...   \n",
       "\n",
       "                        date             user   rt   fav  followers  verified  \\\n",
       "0  2022-07-10 10:16:06+00:00  ibrahim___ethem   20   343     136873     False   \n",
       "1  2022-07-09 20:53:12+00:00  ASIM_YALCINKAYA  107   934     138759     False   \n",
       "2  2022-07-09 07:01:24+00:00  ibrahim___ethem   23   379     136873     False   \n",
       "3  2022-07-08 13:02:50+00:00  ibrahim___ethem   62   665     136873     False   \n",
       "4  2022-07-08 07:14:06+00:00     ____PASA____   79  1647     111568     False   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.rename(columns={'Ger√ßek': 'label'}, inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1222, 1136)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2[df2.label == 1]), len(df2[df2.label == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : 1886 Test : 236 Val : 236\n"
     ]
    }
   ],
   "source": [
    "train_df = df2.sample(frac=0.8, random_state=42)\n",
    "test_df = df2.drop(train_df.index)\n",
    "val_df = test_df.sample(frac=0.5, random_state=42)\n",
    "test_df = test_df.drop(val_df.index)\n",
    "print(\"Train :\", len(train_df), \"Test :\", len(test_df), \"Val :\", len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = train_df.drop(['id', 'rt', 'fav', 'date', 'user', 'followers', 'verified'], axis=1, inplace=False)\n",
    "tst_df = test_df.drop(['id', 'rt', 'fav', 'date', 'user', 'followers', 'verified'], axis=1, inplace=False)\n",
    "vl_df = val_df.drop(['id', 'rt', 'fav', 'date', 'user', 'followers', 'verified'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df.reset_index(drop=True, inplace=True)\n",
    "tst_df.reset_index(drop=True, inplace=True)\n",
    "vl_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.Dataset.from_pandas(tr_df)\n",
    "test_dataset = datasets.Dataset.from_pandas(tst_df)\n",
    "val_dataset = datasets.Dataset.from_pandas(vl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Garanti Yatƒ±rƒ±m, ƒ∞≈ü Bankasƒ± i√ßin tavsiyesini \"endekse paralel getiri\" olarak korudu, yeni hedef fiyatƒ± 12,50 TL  #ISCTR',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/vocab.txt from cache at /Users/damlakonur/.cache/huggingface/transformers/96f3819e738b477201836364517d5979cdbdc6db98ff824a9d1d1918b4ea4cdf.e973deaebec490bbf506dc57141eecbe7c606e78ad5b55fbe9b2c9162abad092\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/tokenizer_config.json from cache at /Users/damlakonur/.cache/huggingface/transformers/1884d6c50b125149343c930759fdcd2bac71b3a4f7181f79d0ba0ac81c927dae.1234e3020e8b22f6151b88ea98a593213c8b28579933530baa777c65097a4e37\n",
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/config.json from cache at /Users/damlakonur/.cache/huggingface/transformers/120e27321f5f101e4616b430bb300523eb0c464006badb271fc4a80ecb3f4551.453a629e781b4c858049daeb69936fc02d2ee7e3314c6c65fa5f432c13470419\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-128k-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/config.json from cache at /Users/damlakonur/.cache/huggingface/transformers/120e27321f5f101e4616b430bb300523eb0c464006badb271fc4a80ecb3f4551.453a629e781b4c858049daeb69936fc02d2ee7e3314c6c65fa5f432c13470419\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-128k-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"dbmdz/bert-base-turkish-128k-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x17acc9280> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.61s/ba]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.85ba/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 39.15ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1886\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DataCollatorWithPadding to create a batch of examples. It will also dynamically pad your text to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the tokenizer function by setting padding=True, dynamic padding is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/config.json from cache at /Users/damlakonur/.cache/huggingface/transformers/120e27321f5f101e4616b430bb300523eb0c464006badb271fc4a80ecb3f4551.453a629e781b4c858049daeb69936fc02d2ee7e3314c6c65fa5f432c13470419\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dbmdz/bert-base-turkish-128k-uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased/resolve/main/pytorch_model.bin from cache at /Users/damlakonur/.cache/huggingface/transformers/3345ec0d05fa890a371a29d54e90dfe6135aa8a16486d037cd5bd1c3665b5614.1568b123824a920fb754ca0cdf98bdaa3254b5cba00f451e08bd7a963a879fff\n",
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds - pred.predictions.argmax(-1)\n",
    "    Precision, Recall, f1, _ = Precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'Accuracy': acc,\n",
    "        'F1': f1,\n",
    "        'Precision': Precision,\n",
    "        'Recall': Recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bc-results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1886\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 590\n",
      "  0%|          | 0/590 [00:00<?, ?it/s]The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      " 20%|‚ñà‚ñà        | 118/590 [29:10<2:07:54, 16.26s/it]***** Running Evaluation *****\n",
      "  Num examples = 236\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/damlakonur/Desktop/bist30-nlp/finetune.ipynb H√ºcre 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/damlakonur/Desktop/bist30-nlp/finetune.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py:1409\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1406\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1408\u001b[0m )\n\u001b[0;32m-> 1409\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1410\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1411\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1412\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1413\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1414\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py:1743\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1740\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1743\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1745\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   1746\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1747\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py:1912\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1910\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 1912\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   1913\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[1;32m   1915\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py:2621\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2618\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2620\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2621\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2622\u001b[0m     eval_dataloader,\n\u001b[1;32m   2623\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2624\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2625\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2626\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2627\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2628\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2629\u001b[0m )\n\u001b[1;32m   2631\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2632\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m   2633\u001b[0m     speed_metrics(\n\u001b[1;32m   2634\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2638\u001b[0m     )\n\u001b[1;32m   2639\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py:2903\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2899\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   2900\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   2901\u001b[0m         )\n\u001b[1;32m   2902\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2903\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   2904\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2905\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32m/Users/damlakonur/Desktop/bist30-nlp/finetune.ipynb H√ºcre 28\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/damlakonur/Desktop/bist30-nlp/finetune.ipynb#Y125sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(pred):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/damlakonur/Desktop/bist30-nlp/finetune.ipynb#Y125sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     labels \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mlabel_ids\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/damlakonur/Desktop/bist30-nlp/finetune.ipynb#Y125sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     preds \u001b[39m-\u001b[39m pred\u001b[39m.\u001b[39mpredictions\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/damlakonur/Desktop/bist30-nlp/finetune.ipynb#Y125sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     Precision, Recall, f1, _ \u001b[39m=\u001b[39m Precision_recall_fscore_support(labels, preds, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/damlakonur/Desktop/bist30-nlp/finetune.ipynb#Y125sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     acc \u001b[39m=\u001b[39m accuracy_score(labels, preds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
